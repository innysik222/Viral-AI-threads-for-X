## üßµ VIRAL THREAD

Post 1: üö® HUGE: OpenAI just dropped a bomb. SWE-Bench Verified? Officially OUT. It's been "saturated" & "contaminated." What's next? ü§Ø #AI #Coding #OpenAI #SWEBenchPro

Post 2: The problem: SWE-Bench Verified, once a gold standard for coding benchmarks, is now just measuring an AI's ability to *guess* what narrow implementation details a human coder would use. ü§¶‚Äç‚ôÇÔ∏è #AIbenchmarks #contamination

Post 3: "It's like everyone in the labs takes turns incrementing 0.1% on the benchmark. Not super convincing." - The brutal honesty is *chef's kiss*. üíØ We're chasing tiny gains on a broken metric.

Post 4: The cost of "verified"? Hundreds of real-world software engineers reviewing problems, triple-checking solutions. It was a *massive* investment, now rendered largely irrelevant. üí∏ #engineering #AIdevelopment

Post 5: Contamination is the killer. Models are "reasoning" about code from open-source repos, effectively cheating.  G5.2 even *added* missing arguments based on repository history! ü§Ø #AIcheating #datascience

Post 6: Enter SWE-Bench Pro:  Larger, harder problems, diverse languages, less contamination. OpenAI is pushing for benchmarks that measure *real* coding capability, not just test-passing. üöÄ #AIfuture

Post 7: Alpha Take: SWE-Bench Verified's demise is a crucial lesson.  Benchmarks *always* become obsolete. The real work is in building *better* benchmarks and focusing on measuring true problem-solving, not just narrow code completion.  What are *your* ideal AI coding challenges? üëá #AIinnovation #futureofwork



## üì∞ SUMMARY ARTICLE

# OpenAI Declares SWE-Bench Verified ‚ÄúSaturated‚Äù and Pushes for SWE-Bench Pro

**TL;DR:** In a surprising announcement, OpenAI researchers Mia Glaese and Olivia Watkins declared that the widely used SWE-Bench Verified coding benchmark has become unreliable due to contamination and saturation. They are advocating for a shift to SWE-Bench Pro, a new benchmark designed to address these shortcomings and provide a more accurate measure of coding model performance.  The move signals a significant evolution in AI evaluation practices.

### üîë Key Insights
*   **SWE-Bench Verified is Obsolete:** The benchmark has become saturated, meaning models are achieving near-perfect scores without demonstrating genuine coding progress.
*   **Contamination is Rampant:** Models are "leaking" knowledge from open-source repositories, allowing them to solve problems with information not present in the original task description.
*   **SWE-Bench Pro Offers a Fresh Start:** The new benchmark features larger, more complex problems, increased diversity in languages, and reduced contamination risk.
*   **Human Evaluation Remains Crucial:** The original SWE-Bench Verified project required extensive human review and curation, highlighting the importance of expert judgment in benchmark development.
*   **Focus is Shifting to Real-World Problem Solving:** OpenAI is emphasizing benchmarks that measure capabilities beyond simple code completion, such as design decisions and maintainability.

### üß† Deep Dive

**The Limits of Benchmarks & the Rise of Contamination**

The interview highlights a common challenge in the AI field: benchmarks, initially valuable for measuring progress, eventually become saturated and lose their predictive power. SWE-Bench Verified, originally intended to assess coding model capabilities, has reached that point.  The issue isn't a lack of effort; the original benchmark was a significant investment by OpenAI, involving hundreds of software engineers to meticulously curate and validate the problem sets. However, the open nature of the benchmark, sourcing problems from public GitHub repositories, has created a significant problem: contamination.  Models are now effectively "cheating" by leveraging knowledge gleaned from these repositories, rendering the benchmark less indicative of true coding ability.

**The Cost of Quality: Human-in-the-Loop Validation**

The interview underscores the significant resources required to create a robust benchmark. The original SWE-Bench Verified project involved a substantial human data campaign, with numerous expert software engineers reviewing problems and solutions multiple times. As Mia Glaese stated, "It was definitely needed to have three reviews‚Ä¶ it was a lot of effort to get there." This emphasizes the limitations of automated evaluation and the ongoing need for human judgment in assessing AI capabilities.  The shift to SWE-Bench Pro aims to address the contamination issue while maintaining a high level of quality.

**Beyond Code Completion: Towards Realistic AI Coding Agents**

OpenAI's move towards SWE-Bench Pro and broader evaluation frameworks like GDP Val (General Purpose Digital Professional) reflects a shift in priorities. The focus is moving beyond simple code completion to measuring more complex and nuanced capabilities. This includes the ability to make sound design decisions, write maintainable code, and solve problems in a way that aligns with human preferences.  As Olivia Watkins noted, "we're starting to look at like much more longer term tasks‚Ä¶does it have like design taste, right? Like does it solve the problem the way that you know my team likes to solve problems?"

**The Future of AI Evaluation**

The interview reveals a growing recognition within OpenAI that benchmarks are not static entities. They evolve, become contaminated, and eventually need to be replaced. The field is moving towards more complex, real-world simulations and incorporating human feedback to create more accurate and meaningful evaluations of AI coding capabilities.  The shift from SWE-Bench Verified to SWE-Bench Pro is a pivotal moment, signaling a commitment to continuous improvement and a deeper understanding of the challenges in assessing AI performance.

### üí¨ Notable Quotes

> ‚ÄúIt‚Äôs like everyone in the labs takes turns incrementing 0.1% on the benchmark. Not super convincing.‚Äù - Mia Glaese, VP of Research at OpenAI

### üèÅ Conclusion

OpenAI's decision to move away from SWE-Bench Verified is a critical moment in the evolution of AI evaluation. It serves as a reminder that benchmarks are tools, not endpoints, and that continuous innovation and adaptation are essential for accurately measuring progress and ensuring the responsible development of AI. The future of AI coding benchmarks will likely involve a greater emphasis on real-world complexity, human feedback, and a constant vigilance against contamination.



IMAGE GENERATION PROMPT: "A futuristic, abstract visualization of a coding benchmark being consumed and transformed into a new, more complex structure. The original benchmark is depicted as a static, crystalline form, while the new benchmark is a dynamic, flowing network of interconnected nodes, with subtle visual cues representing human collaboration and the avoidance of contamination. Use a color palette of cool blues, greens, and hints of gold to convey a sense of progress and innovation. Render in a style reminiscent of a high-tech infographic, with clean lines and a sense of depth."